{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: PCA and Feature Selection\n",
    "\n",
    "## SVMs and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_train = pd.read_csv('sonar_train.data', header=None)\n",
    "sonar_test = pd.read_csv('sonar_test.data', header=None)\n",
    "sonar_valid = pd.read_csv('sonar_valid.data', header=None)\n",
    "\n",
    "sonar_train.loc[sonar_train[60] == 2, 60] = -1\n",
    "sonar_test.loc[sonar_test[60] == 2, 60] = -1\n",
    "sonar_valid.loc[sonar_valid[60] == 2, 60] = -1\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "def split_data(data):\n",
    "    return data.iloc[:, :60].to_numpy(), data.iloc[:, 60:].to_numpy()\n",
    "\n",
    "X, y_train = split_data(sonar_train)\n",
    "train_mean = X.mean(axis=0)\n",
    "print(train_mean)\n",
    "train_std = X.std(axis=0)\n",
    "\n",
    "X_train = normalize(X, train_mean, train_std)\n",
    "X, y_validation = split_data(sonar_valid)\n",
    "X_validation = normalize(X, train_mean, train_std)\n",
    "X, y_test = split_data(sonar_test)\n",
    "X_test = normalize(X, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform PCA on the training data to reduce the dimensionality of the data set (ignoring the\n",
    "class labels for the moment). What are the top six eigenvalues of the data covariance matrix?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance(norm_data):\n",
    "    # Covariance matrix has dimensions (p x p)\n",
    "    # Usually computed with variables as rows and observations as columns (np.cov)\n",
    "    return norm_data.T.dot(norm_data)\n",
    "\n",
    "covariance_mat = compute_covariance(X_train) #np.cov(X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104.        ,  63.80086902,  41.0309772 , ...,  39.88264   ,\n",
       "         37.45343152,  25.68642791],\n",
       "       [ 63.80086902, 104.        ,  73.47996522, ...,  28.92069601,\n",
       "         30.32249592,  21.5262677 ],\n",
       "       [ 41.0309772 ,  73.47996522, 104.        , ...,  29.84603967,\n",
       "         37.01056913,  26.1075827 ],\n",
       "       ...,\n",
       "       [ 39.88264   ,  28.92069601,  29.84603967, ..., 104.        ,\n",
       "         54.27289429,  30.38930996],\n",
       "       [ 37.45343152,  30.32249592,  37.01056913, ...,  54.27289429,\n",
       "        104.        ,  60.76099161],\n",
       "       [ 25.68642791,  21.5262677 ,  26.1075827 , ...,  30.38930996,\n",
       "         60.76099161, 104.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 Eigen values [1344.02076581 1196.11970978  541.52327862  351.33131561  313.13028781\n",
      "  267.04094634]\n"
     ]
    }
   ],
   "source": [
    "e_val, e_vec = np.linalg.eig(covariance_mat)\n",
    "i_rev = e_val.argsort()[::-1]\n",
    "eig_vals = e_val[i_rev]\n",
    "eig_vecs = e_vec[:, i_rev]\n",
    "print(\"Top 6 Eigen values\", eig_vals[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each k ∈ {1, 2, 3, 4, 5, 6}, project the training data into the best k dimensional subspace\n",
    "(with respect to the Frobenius norm) and use the SVM with slack formulation to learn a\n",
    "classifier for each c ∈ {1, 10, 100, 1000}. Report the error of the learned classifier on the\n",
    "validation set for each k and c pair.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'k': [],\n",
    "    'c': [],\n",
    "    'Training Data Error': [],\n",
    "    'Validation Data Error': []\n",
    "}\n",
    "for k in range(1,7):\n",
    "    U = eig_vecs[:,:k]\n",
    "    X_proj = X_train.dot(U)\n",
    "    X_valid_proj = X_validation.dot(U)\n",
    "    for c in C:\n",
    "        data['k'].append(k)\n",
    "        data['c'].append(c)\n",
    "        clf = SVC(C=c, kernel='linear')\n",
    "        clf.fit(X_proj, y_train.ravel())\n",
    "        y_pred = clf.predict(X_proj)\n",
    "        data['Training Data Error'].append(1 - np.mean(y_pred == y_train.ravel()))\n",
    "        valid_pred = clf.predict(X_valid_proj)\n",
    "        data['Validation Data Error'].append(1 - np.mean(valid_pred == y_validation.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>c</th>\n",
       "      <th>Training Data Error</th>\n",
       "      <th>Validation Data Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k     c  Training Data Error  Validation Data Error\n",
       "0   1     1             0.509615               0.461538\n",
       "1   1    10             0.509615               0.461538\n",
       "2   1   100             0.509615               0.461538\n",
       "3   1  1000             0.509615               0.461538\n",
       "4   2     1             0.432692               0.307692\n",
       "5   2    10             0.432692               0.307692\n",
       "6   2   100             0.432692               0.307692\n",
       "7   2  1000             0.432692               0.307692\n",
       "8   3     1             0.269231               0.211538\n",
       "9   3    10             0.269231               0.211538\n",
       "10  3   100             0.269231               0.211538\n",
       "11  3  1000             0.278846               0.211538\n",
       "12  4     1             0.288462               0.211538\n",
       "13  4    10             0.288462               0.211538\n",
       "14  4   100             0.288462               0.211538\n",
       "15  4  1000             0.288462               0.211538\n",
       "16  5     1             0.269231               0.250000\n",
       "17  5    10             0.269231               0.250000\n",
       "18  5   100             0.269231               0.250000\n",
       "19  5  1000             0.269231               0.250000\n",
       "20  6     1             0.240385               0.269231\n",
       "21  6    10             0.240385               0.269231\n",
       "22  6   100             0.240385               0.269231\n",
       "23  6  1000             0.240385               0.269231"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it compare to the best\n",
    "classifier (with the same possible c choices) without feature selection?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>Training Data Error</th>\n",
       "      <th>Validation Data Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c  Training Data Error  Validation Data Error\n",
       "0     1             0.009615               0.211538\n",
       "1    10             0.000000               0.230769\n",
       "2   100             0.000000               0.230769\n",
       "3  1000             0.000000               0.230769"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'c': [],\n",
    "    'Training Data Error': [],\n",
    "    'Validation Data Error': []\n",
    "}\n",
    "for c in C:\n",
    "    data['c'].append(c)\n",
    "    clf = SVC(C=c, kernel='linear', random_state=0)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    y_pred = clf.predict(X_train)\n",
    "    data['Training Data Error'].append(1 - np.mean(y_pred == y_train.ravel()))\n",
    "    valid_pred = clf.predict(X_validation)\n",
    "    data['Validation Data Error'].append(1 - np.mean(valid_pred == y_validation.ravel()))\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the error of the best k/c pair on the test data? How does it compare to the best\n",
    "classifier (with the same possible c choices) without feature selection? Explain your observations.**\n",
    "\n",
    "Best k = 3 and c = 1, 10, 100, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>Test data error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c  Test data error\n",
       "0     1         0.192308\n",
       "1    10         0.192308\n",
       "2   100         0.192308\n",
       "3  1000         0.192308"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = eig_vecs[:,:3]\n",
    "X_proj = X_train.dot(U)\n",
    "X_test_proj = X_test.dot(U)\n",
    "res = {\n",
    "    'c': [],\n",
    "    'Test data error': []\n",
    "}\n",
    "for c in [1, 10, 100, 1000]:\n",
    "    res['c'].append(c)\n",
    "    clf = SVC(C=c, kernel='linear', random_state=0)\n",
    "    clf.fit(X_proj, y_train.ravel())\n",
    "    res['Test data error'].append(1 - np.mean(clf.predict(X_test_proj) == y_test.ravel()))\n",
    "pd.DataFrame.from_dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>Test data error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c  Test data error\n",
       "0  1         0.211538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM without feature selection\n",
    "res = {\n",
    "    'c': [],\n",
    "    'Test data error': []\n",
    "}\n",
    "for c in [1]:\n",
    "    res['c'].append(c)\n",
    "    clf = SVC(C=c, kernel='linear', random_state=0)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    res['Test data error'].append(1 - np.mean(clf.predict(X_test) == y_test.ravel()))\n",
    "pd.DataFrame.from_dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-weight:700\">SVM on reduced dimension data performs better on test data compared to SVM without any feature selection</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you had to pick a value of k before evaluating the performance on the validation set (e.g.,\n",
    "if this was not a supervised learning problem), how might you pick it?**\n",
    "\n",
    "Value of k or the number of components could be picked by heuristics. We can pick the number of components needed to explain at least 85% of the data variance. For this dataset, we can pick k = 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.cumsum(eig_vals) / np.sum(eig_vals), marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Total variance retained')\n",
    "plt.axhline(y=0.85, color='r', linestyle='-')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Feature Selection\n",
    "\n",
    "**1. Compute the top k eigenvalues and eigenvectors of the covariance matrix corresponding to\n",
    "the data matrix omitting the labels (recall that the rows of the data matrix are the input\n",
    "data points).**\n",
    "\n",
    "**2. Define π**\n",
    "\n",
    "**3. Sample s columns independently from the probability distribution defined by π.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_features(features, s, pi):\n",
    "    sel_features = np.random.choice(features, s, p=pi, replace=False)\n",
    "    return sel_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does π define a probability distribution?**\n",
    "\n",
    "π has values ranging between 0 and 1. Also sum of all values in π = 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again, using the UCI Sonar data set, for each k ∈ {1, . . . , 10} and each s ∈ {1, . . . , 20},\n",
    "report the average test error of the SVM with slack classifier over 100 experiments. For each\n",
    "experiment use only the s selected features (note that there may be some duplicates, so only\n",
    "include each feature once).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014c62d86cef4aceb42f4d8b8dac9a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adeb15a94dd485b94c042e063c6a338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2b35e79b5140999734d098a9a00866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acfc3d2742b493287c2eb2cd76214fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49bc85ea9554a108b2c063b88af7ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cf8cdf16a747078e4970e37bfd6bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e2168b52b345cabb8ad371af0069d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b7c04cb5d7478daa840b56a693c0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2af895635e429982e3824afe788c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7630ab1aac6b407590b7446cf3312597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a3446a46f341a4ae7f42a694eeed04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'k': [],\n",
    "    's': [],\n",
    "    'Average test error': []\n",
    "}\n",
    "datapoints, features = X_train.shape\n",
    "for k in trange(1,11):\n",
    "    V = eig_vecs[:k]\n",
    "    pi = np.sum(V**2, axis=0) / k\n",
    "    for s in trange(1,21):\n",
    "        error = []\n",
    "        for _ in range(100):\n",
    "            sel_features = select_k_features(features, s, pi)\n",
    "            clf = SVC(C=1, kernel='linear', random_state=0)\n",
    "            clf.fit(X_train[:,sel_features], y_train.ravel())\n",
    "            y_pred = clf.predict(X_test[:,sel_features])\n",
    "            error.append(1 - np.mean(y_pred==y_test.ravel()))\n",
    "        mean_error = np.mean(error)\n",
    "        data['k'].append(k)\n",
    "        data['s'].append(s)\n",
    "        data['Average test error'].append(mean_error)\n",
    "res = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>s</th>\n",
       "      <th>Average test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.460769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.401154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.378269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.367692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.350192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.320769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.318846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.318077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.305192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.302115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.291731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.285962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.292308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.286154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.285385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.283654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.281731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.267115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.268269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.268269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.435962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.389038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.342885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.343269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.311154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.299808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.296731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.297308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.280962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.267885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.266731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.258269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.259423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.252885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.240192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.247692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.239423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.364231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.347885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.320769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.293654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.294423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.282885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.280769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.265577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.271154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.264231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.252692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.258269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.256731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.248077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.247692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.247692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.259423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.253077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.430385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.376346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.331923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.323077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.305192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.281346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.281731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.280385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.273654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.257308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.256346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.258654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.263077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.255385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.258654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.247692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.250385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>0.245769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0.250962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.247115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.365577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.302115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.295192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.286731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.283077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.271538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.266923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.273269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.258269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.255385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.261538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.250769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.254808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.240962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.246731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.242308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.372308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.341923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.294231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.282308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.282115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.270385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.255577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0.248462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.248654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.244231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.252308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0.252115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0.239231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0.239038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.242885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.430577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.352885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.344808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.313654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.289423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.282885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.274038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.265577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.260385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.254808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.258654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.259038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.259038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.242885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.250192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>0.243846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0.241538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0.244615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0.245192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.384038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.336731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.301346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.289038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.269615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.280385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.274231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.256731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.258269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.253846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.251346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.245769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.248462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.245192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0.236538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.241154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.415577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.373846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.336154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.318462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.289615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.287692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.268846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.275192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.276731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.251731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.247115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.261731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.251346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.248846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.250192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0.234038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.245769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.247308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.252308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.431731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.366154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.300192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.286154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.266923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.262115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.256731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.256731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.264423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.257885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.242308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.247692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.242115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.244038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.235385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.231154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.231731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      k   s  Average test error\n",
       "0     1   1            0.460769\n",
       "1     1   2            0.401154\n",
       "2     1   3            0.378269\n",
       "3     1   4            0.367692\n",
       "4     1   5            0.350192\n",
       "5     1   6            0.320769\n",
       "6     1   7            0.318846\n",
       "7     1   8            0.318077\n",
       "8     1   9            0.305192\n",
       "9     1  10            0.302115\n",
       "10    1  11            0.291731\n",
       "11    1  12            0.285962\n",
       "12    1  13            0.292308\n",
       "13    1  14            0.286154\n",
       "14    1  15            0.285385\n",
       "15    1  16            0.283654\n",
       "16    1  17            0.281731\n",
       "17    1  18            0.267115\n",
       "18    1  19            0.268269\n",
       "19    1  20            0.268269\n",
       "20    2   1            0.435962\n",
       "21    2   2            0.389038\n",
       "22    2   3            0.342885\n",
       "23    2   4            0.343269\n",
       "24    2   5            0.311154\n",
       "25    2   6            0.299808\n",
       "26    2   7            0.296731\n",
       "27    2   8            0.297308\n",
       "28    2   9            0.280962\n",
       "29    2  10            0.265385\n",
       "30    2  11            0.267885\n",
       "31    2  12            0.266731\n",
       "32    2  13            0.258269\n",
       "33    2  14            0.252500\n",
       "34    2  15            0.259423\n",
       "35    2  16            0.252885\n",
       "36    2  17            0.240192\n",
       "37    2  18            0.247500\n",
       "38    2  19            0.247692\n",
       "39    2  20            0.239423\n",
       "40    3   1            0.428077\n",
       "41    3   2            0.364231\n",
       "42    3   3            0.347885\n",
       "43    3   4            0.320769\n",
       "44    3   5            0.293654\n",
       "45    3   6            0.294423\n",
       "46    3   7            0.282885\n",
       "47    3   8            0.280769\n",
       "48    3   9            0.265577\n",
       "49    3  10            0.271154\n",
       "50    3  11            0.264231\n",
       "51    3  12            0.252692\n",
       "52    3  13            0.258269\n",
       "53    3  14            0.256731\n",
       "54    3  15            0.248077\n",
       "55    3  16            0.247692\n",
       "56    3  17            0.247692\n",
       "57    3  18            0.259423\n",
       "58    3  19            0.242500\n",
       "59    3  20            0.253077\n",
       "60    4   1            0.430385\n",
       "61    4   2            0.376346\n",
       "62    4   3            0.331923\n",
       "63    4   4            0.323077\n",
       "64    4   5            0.305192\n",
       "65    4   6            0.281346\n",
       "66    4   7            0.281731\n",
       "67    4   8            0.280385\n",
       "68    4   9            0.273654\n",
       "69    4  10            0.257308\n",
       "70    4  11            0.256346\n",
       "71    4  12            0.258654\n",
       "72    4  13            0.263077\n",
       "73    4  14            0.255385\n",
       "74    4  15            0.258654\n",
       "75    4  16            0.247692\n",
       "76    4  17            0.250385\n",
       "77    4  18            0.245769\n",
       "78    4  19            0.250962\n",
       "79    4  20            0.247115\n",
       "80    5   1            0.434615\n",
       "81    5   2            0.365577\n",
       "82    5   3            0.345577\n",
       "83    5   4            0.302115\n",
       "84    5   5            0.295192\n",
       "85    5   6            0.286731\n",
       "86    5   7            0.283077\n",
       "87    5   8            0.271538\n",
       "88    5   9            0.266923\n",
       "89    5  10            0.273269\n",
       "90    5  11            0.265000\n",
       "91    5  12            0.258269\n",
       "92    5  13            0.255385\n",
       "93    5  14            0.261538\n",
       "94    5  15            0.250769\n",
       "95    5  16            0.254808\n",
       "96    5  17            0.240962\n",
       "97    5  18            0.246731\n",
       "98    5  19            0.242308\n",
       "99    5  20            0.242500\n",
       "100   6   1            0.427692\n",
       "101   6   2            0.372308\n",
       "102   6   3            0.341923\n",
       "103   6   4            0.317500\n",
       "104   6   5            0.294231\n",
       "105   6   6            0.282308\n",
       "106   6   7            0.282115\n",
       "107   6   8            0.267500\n",
       "108   6   9            0.270385\n",
       "109   6  10            0.265385\n",
       "110   6  11            0.260000\n",
       "111   6  12            0.255577\n",
       "112   6  13            0.248462\n",
       "113   6  14            0.248654\n",
       "114   6  15            0.244231\n",
       "115   6  16            0.252308\n",
       "116   6  17            0.252115\n",
       "117   6  18            0.239231\n",
       "118   6  19            0.239038\n",
       "119   6  20            0.242885\n",
       "120   7   1            0.430577\n",
       "121   7   2            0.352885\n",
       "122   7   3            0.344808\n",
       "123   7   4            0.313654\n",
       "124   7   5            0.289423\n",
       "125   7   6            0.282885\n",
       "126   7   7            0.274038\n",
       "127   7   8            0.265577\n",
       "128   7   9            0.260385\n",
       "129   7  10            0.254808\n",
       "130   7  11            0.258654\n",
       "131   7  12            0.259038\n",
       "132   7  13            0.259038\n",
       "133   7  14            0.242885\n",
       "134   7  15            0.250192\n",
       "135   7  16            0.242500\n",
       "136   7  17            0.243846\n",
       "137   7  18            0.241538\n",
       "138   7  19            0.244615\n",
       "139   7  20            0.245192\n",
       "140   8   1            0.428269\n",
       "141   8   2            0.384038\n",
       "142   8   3            0.336731\n",
       "143   8   4            0.330000\n",
       "144   8   5            0.301346\n",
       "145   8   6            0.289038\n",
       "146   8   7            0.269615\n",
       "147   8   8            0.280385\n",
       "148   8   9            0.274231\n",
       "149   8  10            0.256731\n",
       "150   8  11            0.258269\n",
       "151   8  12            0.253846\n",
       "152   8  13            0.262500\n",
       "153   8  14            0.251346\n",
       "154   8  15            0.245769\n",
       "155   8  16            0.246154\n",
       "156   8  17            0.248462\n",
       "157   8  18            0.245192\n",
       "158   8  19            0.236538\n",
       "159   8  20            0.241154\n",
       "160   9   1            0.415577\n",
       "161   9   2            0.373846\n",
       "162   9   3            0.336154\n",
       "163   9   4            0.318462\n",
       "164   9   5            0.297500\n",
       "165   9   6            0.289615\n",
       "166   9   7            0.287692\n",
       "167   9   8            0.268846\n",
       "168   9   9            0.275192\n",
       "169   9  10            0.276731\n",
       "170   9  11            0.251731\n",
       "171   9  12            0.247115\n",
       "172   9  13            0.261731\n",
       "173   9  14            0.251346\n",
       "174   9  15            0.248846\n",
       "175   9  16            0.250192\n",
       "176   9  17            0.234038\n",
       "177   9  18            0.245769\n",
       "178   9  19            0.247308\n",
       "179   9  20            0.252308\n",
       "180  10   1            0.431731\n",
       "181  10   2            0.366154\n",
       "182  10   3            0.333269\n",
       "183  10   4            0.310000\n",
       "184  10   5            0.300192\n",
       "185  10   6            0.286154\n",
       "186  10   7            0.285769\n",
       "187  10   8            0.266923\n",
       "188  10   9            0.262115\n",
       "189  10  10            0.256731\n",
       "190  10  11            0.256731\n",
       "191  10  12            0.264423\n",
       "192  10  13            0.257885\n",
       "193  10  14            0.242308\n",
       "194  10  15            0.247692\n",
       "195  10  16            0.242115\n",
       "196  10  17            0.244038\n",
       "197  10  18            0.235385\n",
       "198  10  19            0.231154\n",
       "199  10  20            0.231731"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>s</th>\n",
       "      <th>Average test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.231154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      k   s  Average test error\n",
       "198  10  19            0.231154"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc[res['Average test error'] == res['Average test error'].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does this provide a reasonable alternative to SVM with slack formulation without feature\n",
    "selection on this data set? What are the pros and cons of this approach?**\n",
    "\n",
    "SVM with feature selection (k = 7,  s = 20, c = 1) gives lowest average test error of 23% whereas SVM without feature selection  (c = 1) gives lowest test error of 21%. There is not much improvement with feature selection.\n",
    "\n",
    "Pros:\t\n",
    "- Training individual models is computationally faster as number of features is less\n",
    "\n",
    "Cons:\n",
    "- Choosing k and s is difficult. Grid search like above takes long time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: VC Dimension\n",
    "\n",
    "2. Consider a binary classification problem for data points in R. Let H be the hypothesis space\n",
    "of all intervals in R. Given an interval in H, points inside the interval are classified as '+'\n",
    "and the remaining points are classified as '−'.\n",
    "\n",
    "Consider the boosted hypothesis space H' that takes a pair of hypotheses from H and takes the sign of their weighted combination (similar to what would be produced by two rounds of boosting). Specifically,\n",
    "\n",
    "H' = {f|f(x) = sign(α1h1(x) + α2h2(x)) for some h1, h2 ∈ H and α1, α2 ∈ R}.\n",
    "\n",
    "\n",
    "To break ties, if α1h1(x)+α2h2(x) = 0, the hypothesis should return a '+'. What is VC(H')? Prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VC(H') = 2**\n",
    "\n",
    "**Consider 3 points in one dimension placed as +, -, +.** \n",
    "\n",
    "**2 Rounds of Boosting cannot shatter the points in 1 dimension as after first round one point would remain misclassified and increase the weight of the misclassified point. But in the next iteration, previously low weight point would be misclassified which would again increase weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908.6550430614061\n"
     ]
    }
   ],
   "source": [
    "vc = 5\n",
    "d = 1-.95\n",
    "e = 0.2\n",
    "\n",
    "m = (1/e) * ((4 * math.log(2/d)) + (8 * vc * math.log(13/e)))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'x': [0,1,2],\n",
    "    'y': [1,-1,1]\n",
    "}\n",
    "df_3p = pd.DataFrame.from_dict(data)\n",
    "y_train = df_3p['y']\n",
    "X_train = df_3p['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1]\n",
      " [ 1  1 -1]\n",
      " [ 1 -1 -1]\n",
      " [-1  1  1]\n",
      " [-1  1 -1]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "def gen_hypotheses():\n",
    "    y_val = [1,-1]\n",
    "    H = []\n",
    "    for i in y_val:\n",
    "        for j in y_val:\n",
    "            for k in y_val:\n",
    "                if not (i == k == 1 and j == -1):\n",
    "                    h = [i,j,k]\n",
    "                    H.append(h)\n",
    "    return H\n",
    "H = np.array(gen_hypotheses())\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(H, y_train, m, T):\n",
    "    n = 1\n",
    "    w = np.array([1/m] * m)\n",
    "    alphas = [0] * T\n",
    "    epsilons = [0] * T\n",
    "    selected_H = [None] * T\n",
    "    y_predictions = [None] * T\n",
    "    print(\"Running Adaboost\")\n",
    "    for t in range(T):\n",
    "        e_t = 1\n",
    "        h_t = None\n",
    "        y_t = None\n",
    "        best_i = 0\n",
    "        h_i = 0\n",
    "        tq = tqdm(H)\n",
    "        tq.set_description(f\"Round {t+1}\")\n",
    "        for h in tq:\n",
    "            h_i += 1\n",
    "            y_pred = h\n",
    "            mask = (y_pred != y_train).astype(np.float64)\n",
    "            e_h = np.sum(mask * w)\n",
    "            if e_h < e_t:\n",
    "                e_t = e_h\n",
    "                h_t = h\n",
    "                y_t = y_pred\n",
    "                best_i = h_i\n",
    "        print(f\"Round {t+1} - Best hypothesis index {best_i} {h_t}\")\n",
    "        selected_H[t] = h_t\n",
    "        y_predictions[t] = y_t\n",
    "        epsilons[t] = e_t\n",
    "        \n",
    "        a_t = 0.5 * math.log((1-e_t)/e_t) # Log base e\n",
    "        alphas[t] = a_t\n",
    "        \n",
    "        # Weight update\n",
    "        normalize = 2 * np.sqrt(e_t * (1-e_t))\n",
    "        w = w * np.exp(-1 * y_train * y_t * a_t)/normalize\n",
    "        print(\"Weights: \")\n",
    "        print(y_train.ravel())\n",
    "        print(w.ravel())\n",
    "    return np.array(alphas), np.array(epsilons), selected_H, np.array(y_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Adaboost\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662096eddc074d2ea1b94280d6d715dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1 - Best hypothesis index 1 [1 1 1]\n",
      "Weights: \n",
      "[ 1 -1  1]\n",
      "[0.25 0.5  0.25]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7cbcb7ab0e4f7aa51eca8079135300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 2 - Best hypothesis index 3 [ 1 -1 -1]\n",
      "Weights: \n",
      "[ 1 -1  1]\n",
      "[0.16666667 0.33333333 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "a, e, h_, y_ = adaboost(H, y_train, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_predict(a, H=None, x=None, h_x=None):\n",
    "    if h_x is None:\n",
    "        h_x = []\n",
    "        for h in H:\n",
    "            y_pred = h\n",
    "            h_x.append(y_pred)\n",
    "        h_x = np.array(h_x)\n",
    "    res = a.dot(h_x)\n",
    "    res[res == 0] = 1\n",
    "    return np.sign(res)\n",
    "\n",
    "def accuracy(y_truth, y_pred):\n",
    "    return np.mean(y_truth == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: \n",
      "[0.34657359 0.54930614]\n",
      "Epsilon: \n",
      "[0.33333333 0.25      ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Alpha: \")\n",
    "print(a)\n",
    "print(\"Epsilon: \")\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "pred = boosting_predict(np.array(a), h_x=y_)\n",
    "print(\"Train accuracy: \", accuracy(y_train.ravel(), pred.flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Medical Diagnostics\n",
    "\n",
    "**2. Now, suppose that the hypothesis space consists of only height 1 decision trees for this data\n",
    "set (only one attribute split).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_train = pd.read_csv('heart_train.data', header=None)\n",
    "heart_test = pd.read_csv('heart_test.data', header=None)\n",
    "\n",
    "# Changing 0 class to -1 to predict using Sign function\n",
    "heart_train.loc[heart_train[0] == 0, 0] = -1\n",
    "heart_test.loc[heart_test[0] == 0, 0] = -1\n",
    "\n",
    "# Split X, Y\n",
    "y_train, X_train = heart_train.iloc[:, 0], heart_train.iloc[:, 1:]\n",
    "y_test, X_test = heart_test.iloc[:, 0], heart_test.iloc[:, 1:]\n",
    "\n",
    "attributes = X_train.columns\n",
    "m, n = X_train.shape\n",
    "classes = y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_1_attr_hypotheses(attributes, classes):\n",
    "    hypotheses = []\n",
    "    for l0 in attributes:\n",
    "        for leaf1 in classes:\n",
    "            for leaf2 in classes:\n",
    "                h = {}\n",
    "                h[l0] = {}\n",
    "                h[l0][0] = leaf1\n",
    "                h[l0][1] = leaf2\n",
    "                hypotheses.append(h)\n",
    "    return hypotheses\n",
    "\n",
    "H = generate_1_attr_hypotheses(attributes, classes)\n",
    "assert len(attributes) * (2**2) == len(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(a) Use coordinate descent to minimize the exponential loss function for this hypothesis\n",
    "space over the training set. You can use any initialization and iteration order that you\n",
    "would like other than the one selected by adaBoost. What is the optimal value of Î± that\n",
    "you arrived at? What is the corresponding value of the exponential loss on the training\n",
    "set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, h):\n",
    "    for l0 in h.keys():\n",
    "        val = X[l0]\n",
    "        pred = h[l0][val]\n",
    "        if not isinstance(pred, dict):\n",
    "            return pred\n",
    "        else:\n",
    "            for l1 in pred.keys():\n",
    "                val = X[l1]\n",
    "                return pred[l1][val]\n",
    "            \n",
    "\n",
    "def boosting_predict(a, H=None, x=None, h_x=None):\n",
    "    if h_x is None:\n",
    "        h_x = []\n",
    "        for h in H:\n",
    "            y_pred = x.apply(lambda row: predict(row, h), axis=1)\n",
    "            h_x.append(y_pred)\n",
    "        h_x = np.array(h_x)\n",
    "    return np.sign(a.dot(h_x))\n",
    "\n",
    "def accuracy(y_truth, y_pred):\n",
    "    return np.mean(y_truth == y_pred) * 100\n",
    "\n",
    "def coordinate_descent(y_train, ht_x, H):\n",
    "    def compute_loss(t_prime, y_train, ht_x, H):\n",
    "        loss_n = loss_d = 0\n",
    "        for m in range(len(y_train)):\n",
    "            inner_sum = 0\n",
    "            for t in range(T):\n",
    "                if t_prime != t:\n",
    "                    a_t = alphas[t]\n",
    "                    inner_sum += a_t * ht_x[t][m]\n",
    "            y_t_prime = ht_x[t_prime][m]\n",
    "            if y_t_prime == y_train[m]:\n",
    "                loss_n += np.exp(-1 * y_train[m] * inner_sum)\n",
    "            else:\n",
    "                loss_d += np.exp(-1 * y_train[m] * inner_sum)\n",
    "        return loss_n, loss_d\n",
    "    T = len(H)\n",
    "    alphas = np.array([1/len(y_train)] * T)\n",
    "    alpha_change = 1\n",
    "    changes = []\n",
    "    iter_counter = 0\n",
    "    while alpha_change > 0.01:\n",
    "        start_alphas = np.copy(alphas)\n",
    "        alpha_change = 0\n",
    "        for t_prime in range(T):\n",
    "            loss_n, loss_d = compute_loss(t_prime, y_train, ht_x, H)\n",
    "            alpha_t_prime = 0.5 * np.log(loss_n / loss_d)\n",
    "            alphas[t_prime] = alpha_t_prime\n",
    "        for i in range(len(alphas)):\n",
    "            alpha_change += abs(alphas[i] - start_alphas[i])\n",
    "        if iter_counter % 100 == 0:\n",
    "            print(f\"Iteration {iter_counter} - Alpha change {alpha_change}\")\n",
    "        iter_counter += 1\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_x = []\n",
    "for h in H:\n",
    "    y_pred = X_train.apply(lambda row: predict(row, h), axis=1)\n",
    "    ht_x.append(y_pred)\n",
    "ht_x = np.array(ht_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = coordinate_descent(y_train, ht_x, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha values\n",
      "[ 0.0125     -0.16432002  0.0125     -0.03718624  0.0125     -0.14933684\n",
      "  0.0125     -0.10099259  0.0125     -0.18058637  0.0125     -0.08432352\n",
      "  0.0125     -0.17827792  0.0125     -0.10761323  0.0125      0.04683466\n",
      "  0.0125      0.02717568  0.0125     -0.0697316   0.0125     -0.05131991\n",
      "  0.0125     -0.19045047  0.0125     -0.08232813  0.0125     -0.19724345\n",
      "  0.0125     -0.08344875  0.0125      0.0079193   0.0125      0.00947466\n",
      "  0.0125      0.03249193  0.0125      0.02045813  0.0125     -0.1273416\n",
      "  0.0125     -0.06974654  0.0125     -0.05772459  0.0125     -0.03036459\n",
      "  0.0125     -0.2920413   0.0125     -0.11183699  0.0125      0.09516313\n",
      "  0.0125      0.06324829  0.0125     -0.02606898  0.0125     -0.02301836\n",
      "  0.0125     -0.08694109  0.0125     -0.05881137  0.0125     -0.09471794\n",
      "  0.0125     -0.08430297  0.0125     -0.03309757  0.0125     -0.03110784\n",
      "  0.0125      0.02535473  0.0125      0.02055283  0.0125     -0.16226708\n",
      "  0.0125     -0.10102695  0.0125      0.07688006  0.0125      0.04358646\n",
      "  0.0125     -0.17718393  0.0125     -0.07098222]\n",
      "***********************************************************\n",
      "Training Loss:  60.2226183657083\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Alpha values\")\n",
    "print(alphas)\n",
    "loss = np.sum(np.exp(-y_train * alphas.dot(ht_x)))\n",
    "\n",
    "print(\"***********************************************************\")\n",
    "print(\"Training Loss: \", loss)\n",
    "print(\"***********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(b) What is the accuracy of the resulting classifier on the test data?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************************\n",
      "Coordinate Test Accuracy 69.5187165775401\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "coor_pred = boosting_predict(alphas, H=H, x=X_test)\n",
    "print(\"***********************************************************\")\n",
    "print(\"Coordinate Test Accuracy\", accuracy(y_test.ravel(), coor_pred.flatten()))\n",
    "print(\"***********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(c) What is the accuracy of adaBoost after 20 rounds for this hypothesis space on the test\n",
    "data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Adaboost\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5fb0cf52974445b17fb1b2b4defdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1 - Best hypothesis index 51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15c3f85a5fb4a408a87c4ebcf5a8fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 2 - Best hypothesis index 43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474a50b4b3544804b0f9062fb52bcd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 3 - Best hypothesis index 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311d953406f54875b5430d7fb92572dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 4 - Best hypothesis index 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8760ff6bdd4433a39b750268cbe19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 5 - Best hypothesis index 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765ffff8e6054defbd6464f843be4cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 6 - Best hypothesis index 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39ddb3649ef43f39126a350b70f48f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 7 - Best hypothesis index 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acf7a1032a44062bac833bb5fdb2882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 8 - Best hypothesis index 87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eebf0421404d01ac124b416f3bc193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 9 - Best hypothesis index 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be44e24025e043a08c016aceaf05b1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 10 - Best hypothesis index 63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de20ef02f23f4eebba39450f7b5da10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 11 - Best hypothesis index 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41482cb8efc8436189b189af14e4a190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 12 - Best hypothesis index 79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58131963016744ec83e98f8b165e9315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 13 - Best hypothesis index 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed7077a17f94552bd074fbdc189e55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 14 - Best hypothesis index 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e4b68f612a4bf78e8dbbd1bd322c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 15 - Best hypothesis index 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60466d1a45d7405aa8be9e5ed12f192a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 16 - Best hypothesis index 43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370d1c42036d4ca58f5d9ff9cc4598e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 17 - Best hypothesis index 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1919522dcb3448cb98d4dc97cad05b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 18 - Best hypothesis index 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21adbba0dbc41f5a32eba2868f561b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 19 - Best hypothesis index 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c439f81634435b96deabd9a465276a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 20 - Best hypothesis index 67\n"
     ]
    }
   ],
   "source": [
    "def adaboost(H, X_train, y_train):\n",
    "    m, n = X_train.shape\n",
    "    w = np.array([1/m] * m)\n",
    "    alphas = [0] * T\n",
    "    epsilons = [0] * T\n",
    "    selected_H = [None] * T\n",
    "    y_predictions = [None] * T\n",
    "    best_idxs = []\n",
    "    print(\"Running Adaboost\")\n",
    "    for t in range(T):\n",
    "        e_t = 1\n",
    "        h_t = None\n",
    "        y_t = None\n",
    "        best_i = 0\n",
    "        h_i = 0\n",
    "        tq = tqdm(H)\n",
    "        tq.set_description(f\"Round {t+1}\")\n",
    "        for h in tq:\n",
    "            h_i += 1\n",
    "            y_pred = X_train.apply(lambda row: predict(row, h), axis=1)\n",
    "            mask = (y_pred != y_train).astype(np.float64)\n",
    "            e_h = np.sum(mask * w)\n",
    "            if e_h < e_t:\n",
    "                e_t = e_h\n",
    "                h_t = h\n",
    "                y_t = y_pred\n",
    "                best_i = h_i\n",
    "        print(f\"Round {t+1} - Best hypothesis index {best_i}\")\n",
    "        best_idxs.append(best_i)\n",
    "        selected_H[t] = h_t\n",
    "        y_predictions[t] = y_t\n",
    "        epsilons[t] = e_t\n",
    "        \n",
    "        a_t = 0.5 * math.log((1-e_t)/e_t) # Log base e\n",
    "        alphas[t] = a_t\n",
    "        \n",
    "        # Weight update\n",
    "        normalize = 2 * np.sqrt(e_t * (1-e_t))\n",
    "        w = w * np.exp(-1 * y_train * y_t * a_t)/normalize\n",
    "    return np.array(alphas), np.array(epsilons), selected_H, np.array(y_predictions), best_idxs\n",
    "\n",
    "T = 20\n",
    "a, e, h_, y_, idxs = adaboost(H, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************************\n",
      "AdaBoost Test Accuracy 66.84491978609626\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "pred = boosting_predict(a, H=h_, x=X_test)\n",
    "print(\"***********************************************************\")\n",
    "print(\"AdaBoost Test Accuracy\", accuracy(y_test.ravel(), pred.flatten()))\n",
    "print(\"***********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the Î± learned by adaBoost compare to the one learned by coordinate descent/gradient\n",
    "descent?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(d) Use bagging, with 20 bootstrap samples, to produce an average classifier for this data\n",
    "set. How does it compare to the previous classifiers in terms of accuracy on the test set?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decision_stump(data, attributes):\n",
    "    m, n = data.shape\n",
    "    best_accuracy = 0\n",
    "    best_h = 0\n",
    "    for h in H:\n",
    "        y_pred = data.apply(lambda row: predict(row, h), axis=1)\n",
    "        acc = accuracy(data[0].ravel(), y_pred.ravel())\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_h = h\n",
    "    return best_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 20\n",
    "# Random forest\n",
    "T = []\n",
    "attrs = attributes.to_list()\n",
    "for b in range(B):\n",
    "    bootstrap_sample = heart_train.iloc[np.random.randint(m, size=m)]\n",
    "#     T_b, best_split = fit_decision_stump_ig(bootstrap_sample, attrs)\n",
    "    T_b = fit_decision_stump(bootstrap_sample, attrs)\n",
    "    T.append(T_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************************\n",
      "Bagging Test accuracy 61.49732620320856\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "def stump_predict(T_b, x_row):\n",
    "    for root in T_b.keys():\n",
    "        val = x_row[root]\n",
    "        return T_b[root][val]\n",
    "\n",
    "def random_forest_predict(T, data):\n",
    "    m, n = data.shape\n",
    "    y_pred = np.array([0] * m)\n",
    "    for i in range(m):\n",
    "        row = data.loc[i, :]\n",
    "        preds = defaultdict(int)\n",
    "        for t_b in T:\n",
    "            preds[stump_predict(t_b, row)] += 1\n",
    "        if preds[-1] > preds[1]:\n",
    "            y_pred[i] = -1\n",
    "        else:\n",
    "            y_pred[i] = 1\n",
    "    return y_pred\n",
    "\n",
    "test_accuracy = accuracy(random_forest_predict(T, X_test), y_test.ravel()) \n",
    "\n",
    "print(\"***********************************************************\")\n",
    "print(\"Bagging Test accuracy\", test_accuracy)\n",
    "print(\"***********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(e) Which of these 3 methods should be preferred for this data set and why**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost and Coordinate descent methods can be used for this data set with this hypothesis space. Decision stumps are not expressive, so bagging would not be much useful for improving the accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
